{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install plotly\n",
    "!pip install autoviz\n",
    "!pip install dataprep\n",
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPMM37lhSS8",
    "tags": []
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSlyA6zpVNei",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeL1Ukamg_BR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating train and test data sets (from GitHub, no need to add it manually)\n",
    "url = 'https://raw.githubusercontent.com/AnastasiaDv491/AA-datasets/main/train.csv'\n",
    "\n",
    "full_data_set = pd.read_csv(url)\n",
    "\n",
    "target_price = full_data_set['target']\n",
    "feature_full = full_data_set.drop('target', axis=1)\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(feature_full, target_price, random_state=1)\n",
    "\n",
    "matrix_train = feature_train.join(target_train,on=\"property_id\")\n",
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK5pijjxhQ3M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "full_data_set.head()\n",
    "full_data_set.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SLjt5KOjT0n",
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA) (DO NOT RUN, VERY SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y1sqJ1di_Qr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_data_set.shape     ##(6495, 55)\n",
    "\n",
    "## looking at a few measures\n",
    "\n",
    "full_data_set.describe()\n",
    "\n",
    "full_data_set.nunique()\n",
    "\n",
    "full_data_set['host_response_time'].unique()\n",
    "\n",
    "full_data_set.isnull().sum()  ## quite a few variables can be dropped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYvFc1WQjtHy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## variables that need to be dropped, shall be commented out below\n",
    "\n",
    "df = full_data_set[['property_id', 'property_name', 'property_summary', 'property_space',\n",
    "       'property_desc', 'property_neighborhood', 'property_notes',\n",
    "       'property_transit', 'property_access', 'property_interaction',\n",
    "       'property_rules', 'property_zipcode', 'property_lat', 'property_lon',\n",
    "       'property_type', 'property_room_type', 'property_max_guests',\n",
    "       'property_bathrooms', 'property_bedrooms', 'property_beds',\n",
    "       'property_bed_type', 'property_amenities', 'property_sqfeet',\n",
    "       'property_scraped_at', 'property_last_updated', 'host_id', 'host_since',\n",
    "       'host_location', 'host_about', 'host_response_time',\n",
    "       'host_response_rate', 'host_nr_listings', 'host_nr_listings_total',\n",
    "       'host_verified', 'booking_price_covers', 'booking_min_nights',\n",
    "       'booking_max_nights', 'booking_availability_30',\n",
    "       'booking_availability_60', 'booking_availability_90',\n",
    "       'booking_availability_365', 'booking_cancel_policy', 'reviews_num',\n",
    "       'reviews_first', 'reviews_last', 'reviews_rating', 'reviews_acc',\n",
    "       'reviews_cleanliness', 'reviews_checkin', 'reviews_communication',\n",
    "       'reviews_location', 'reviews_value', 'reviews_per_month', 'extra',\n",
    "       'target']].copy()\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "Qi0K1ZgMmELB",
    "outputId": "155e9af8-a4a4-4ffb-ed69-0c4eb80fe181",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## Relationship analysis\n",
    "\n",
    "correlation = df.corr()\n",
    "sns.heatmap(correlation, xticklabels=correlation.columns, yticklabels=correlation.columns, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw3iBowfnY8C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg_i7_avknlM"
   },
   "source": [
    "From here on we will proceed with only the train features in order to avoid any data spillage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgUpG0dDprFf"
   },
   "source": [
    "## Autoviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wg3Rm-Dz1wZ"
   },
   "source": [
    "[Documentation](https://www.kaggle.com/general/233832)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wI-7M-09jXdF",
    "outputId": "ebc64b01-4b12-46eb-c9ad-1c30d7529368",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing Autoviz class\n",
    "from autoviz.AutoViz_Class import AutoViz_Class #Instantiate the AutoViz class\n",
    "AV = AutoViz_Class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5eUbWA4yldoD",
    "outputId": "e32c19f0-8f9b-4fc9-93aa-444ed774549b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eda=AV.AutoViz(filename=\"\",dfte=matrix_train, chart_format='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_feature_train = AV.AutoViz(filename=\"\",dfte=feature_train, chart_format='html')\n",
    "eda_feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_target_train = AV.AutoViz(filename=\"\",dfte=target_train, chart_format='html')\n",
    "eda_target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA using DataPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import create_report\n",
    "\n",
    "target_report = create_report(target_train)\n",
    "target_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_report.show_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_report = create_report(feature_train)\n",
    "feature_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_report.show_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_report = create_report(matrix_train)\n",
    "matrix_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_report.show_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egMFmRx5q4B8"
   },
   "source": [
    "## Ydata-profling (Former pandas-profling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i9dwPcWz-ZA"
   },
   "source": [
    "[Documentation](https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html#using-inside-jupyter-notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ldRGMcTrB9t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Al0RjOtyrfy1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(matrix_train,title=\"Pandas Profiling Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRAyswxMsBs4"
   },
   "source": [
    "### Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "cXLFMgiZsIHL",
    "outputId": "2d4701a7-88bf-4127-d7d2-2dc9611e8a25",
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp48PExAsGGx"
   },
   "source": [
    "### HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "8UK9kt5vzHu1",
    "outputId": "969d657b-1891-4bdf-e3ae-8cdb1db4c4ee"
   },
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()\n",
    "profile.to_file(\"Ydata_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "eh9BzwdX34oX",
    "outputId": "42d065c1-6aad-4a44-8381-0a7a8ecac9c1"
   },
   "outputs": [],
   "source": [
    "profile.to_file(\"Ydata_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwkSiCcghMhv",
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Puz6mw2NRBz7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pipeline for feature processing (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5STLKD_7fHK9",
    "outputId": "d9fdcd41-1971-4e2c-fdb7-717f5e3b4d5a"
   },
   "outputs": [],
   "source": [
    "# NOT TO USE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = feature_train.copy()\n",
    "y = target_train.copy()\n",
    "\n",
    "class isMissing():\n",
    "  def __init__(self, columns=None, new_columns = None):\n",
    "    self.columns = columns\n",
    "    self.new_columns=new_columns\n",
    "  def fit(self, X, y=None):\n",
    "      return self\n",
    "  \n",
    "  def transform(self,X, y= None):\n",
    "    cols_to_transform = list(self.columns)\n",
    "    new_cols = list(self.new_columns)\n",
    "  \n",
    "    X[new_cols] = np.where(X[cols_to_transform].isna(), 1, 0)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All df to lower case: Pipe\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class toLowerCase():\n",
    "  def __init__(self, columns=None):\n",
    "    self.columns = columns\n",
    "    \n",
    "  def fit(self, X, y=None):\n",
    "      return self\n",
    "  \n",
    "  def transform(self,X, y= None):\n",
    "    X = X.apply(lambda x: x.str.lower() if x.dtype=='object' else x)\n",
    "\n",
    "    return X\n",
    "  \n",
    "def splitWords(df, cols):\n",
    "   df[cols] = df[cols].str.split().str.len()\n",
    "   return df\n",
    "\n",
    "pipe = Pipeline(\n",
    "     steps=[\n",
    "        (\"miss\",isMissing(columns = ['property_name'], new_columns=['property_name_miss'])),\n",
    "        (\"lowercase\", toLowerCase()),\n",
    "        ('splitCount', FunctionTransformer(splitWords,kw_args={'cols':['property_name', 'property_summary']}))\n",
    "    ]\n",
    ")\n",
    "df = pipe.fit_transform(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWords(df, cols):\n",
    "   df[cols] = df[cols].str.split().str.len()\n",
    "   return df\n",
    "\n",
    "splitWords(feature_train, 'property_summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4o4-JLV1csx",
    "tags": []
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "O3dwKNe-xvhM",
    "outputId": "8c3b58a7-c652-4f7d-f831-69905b4b7c28",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping this would be the same as the word doc table\n",
    "feature_train = feature_train.drop(['property_id','property_space','property_desc','property_neighborhood','property_notes','property_access',\n",
    "                                    \"property_interaction\",\"property_rules\",\"host_location\",\"host_about\",\"host_id\",\"property_sqfeet\"], axis = 1)\n",
    "feature_train = feature_train.drop([\"property_transit\"],axis=1)\n",
    "transformed_train = pd.DataFrame()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gDYy-zH8C_wT",
    "outputId": "fedf0313-e092-429c-80a1-a98907acbc6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All df to lower case -works\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def toLower(df):\n",
    "    df = df.apply(lambda x: x.str.lower() if x.dtype=='object' else x)\n",
    "    return df\n",
    "\n",
    "toLower = FunctionTransformer(toLower)\n",
    "toLower.fit_transform(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-lgZs-gGd9S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Property name: replaced by the word count & new column:missing? - works\n",
    "\n",
    "feature_train['property_name_miss'] = np.where(feature_train['property_name'].isna(), 1, 0)\n",
    "# Should refactor this because if run twice it throws an error\n",
    "feature_train.property_name = feature_train['property_name'].str.split().str.len()\n",
    "feature_train.property_name = feature_train.property_name.fillna(0)\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train.property_name],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,feature_train.property_name_miss],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pCUDZaQJWXz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Property summary: new columns - summary_missing & property_summary_count\n",
    "## Missing = 'nan' or one word in summary\n",
    "str_df  = pd.DataFrame()\n",
    "str_df['condition'] = feature_train['property_summary'].str.match(r'\\A[\\w-]+\\Z')\n",
    "\n",
    "feature_train = pd.merge(str_df, feature_train, left_index=True, right_index=True)\n",
    "feature_train['property_summary_miss'] = np.where(feature_train['condition'].isna(), 1, 0)\n",
    "feature_train = feature_train.drop(['condition'], axis = 1)\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train.property_summary_miss],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZT08_2_KH3S",
    "outputId": "078d7937-3eb1-444a-f3c0-7c41ca498cab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Property_summary_count: remove numbers, punctuation \n",
    "feature_train['property_summary'] = feature_train['property_summary'].str.replace('\\d+', '')\n",
    "feature_train['property_summary'] = feature_train['property_summary'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "feature_train['property_summary_count'] = feature_train['property_summary'].str.split().str.len()\n",
    "feature_train = feature_train.drop(['property_summary'], axis = 1)\n",
    "feature_train.property_summary_count = feature_train.property_summary_count.fillna(0)\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train.property_summary_count],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9olRvfdScwOv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_zipcode: replaced with mode\n",
    "zipcode_mode=feature_train['property_zipcode'].mode()[0]\n",
    "feature_train['property_zipcode'] = feature_train['property_zipcode'] .fillna(feature_train['property_zipcode'].mode()[0])\n",
    "\n",
    "feature_train.loc[feature_train['property_zipcode']=='11 20','property_zipcode']=1120\n",
    "\n",
    "freq_table=feature_train['property_zipcode'].value_counts()\n",
    "\n",
    "#encoding: preserve 11 most frequent categories\n",
    "feature_train.property_zipcode=feature_train.property_zipcode.astype(int)\n",
    "zipcode_cats = pd.get_dummies(feature_train.property_zipcode)\n",
    "sorted_zipcode_cats = zipcode_cats[zipcode_cats.sum().sort_values(ascending=False).index]\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,sorted_zipcode_cats.iloc[:,:10]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XM3EHxBjFBbw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_type: regrouped into - apartment, house, other\n",
    "threshold_percent = 3\n",
    "\n",
    "series = pd.value_counts(feature_train['property_type'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)\n",
    "\n",
    "feature_train = feature_train.assign(property_type = np.where(feature_train['property_type'].isin(series[mask].index),'Other', feature_train['property_type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encode property_type\n",
    "type_cats = pd.get_dummies(feature_train['property_type'])\n",
    "transformed_train = pd.concat([transformed_train,type_cats],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVVdqqzHIoib",
    "outputId": "e890de89-01a6-4120-856d-d8c349b4b5b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_bathrooms: regroup into 1. None 2. One 3. More than one\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bathrooms_mode = feature_train['property_bathrooms'].mode()\n",
    "feature_train['property_bathrooms'] =feature_train['property_bathrooms'].fillna(feature_train['property_bathrooms'].mode())\n",
    "\n",
    "np.unique(feature_train['property_bathrooms'],return_counts=True)\n",
    "\n",
    "feature_train['property_bathrooms_cat']= 'One'\n",
    "feature_train.loc[(feature_train['property_bathrooms'] < 1) , 'property_bathrooms_cat'] = 'None'\n",
    "feature_train.loc[(feature_train['property_bathrooms'] > 1), 'property_bathrooms_cat'] = 'More than one'\n",
    "\n",
    "feature_train = feature_train.drop(['property_bathrooms'], axis = 1)\n",
    "feature_train['property_bathrooms_cat']\n",
    "\n",
    "bathroom_cats = pd.get_dummies(feature_train['property_bathrooms_cat'])\n",
    "transformed_train = pd.concat([transformed_train,bathroom_cats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-LkSfEi8RLhs",
    "outputId": "ee353fab-82bb-4b48-e1c0-33a1fd8233cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_bedrooms\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def NumToCategory(df, column,new_column, missing_value = 0):\n",
    "  df[column] =df[column].fillna(missing_value)\n",
    "\n",
    "  df.loc[(df[column] == 1), new_column] = 'One'\n",
    "  df.loc[(df[column] == 2), new_column] = 'Two'\n",
    "  df.loc[(df[column] == 3), new_column] = 'Three'\n",
    "  df.loc[(df[column] > 3), new_column] = 'More than three'\n",
    "\n",
    "  df = df.drop([column], axis = 1)\n",
    "  return df\n",
    "\n",
    "feature_train = NumToCategory(feature_train, 'property_beds', 'property_beds_cat')\n",
    "feature_train = NumToCategory(feature_train, 'property_bedrooms', 'property_bedrooms_cat')\n",
    "\n",
    "beds_dummies = pd.get_dummies(feature_train['property_beds_cat'])\n",
    "bedrooms_dummies = pd.get_dummies(feature_train['property_bedrooms_cat'])\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,beds_dummies,bedrooms_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHnFSmNChhgP",
    "outputId": "3761aa15-12f9-4840-afc0-fd2b03fa4fa4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_bed_type: ok\n",
    "feature_train['property_bed_type'].unique()\n",
    "# room type: ok\n",
    "pd.value_counts(feature_train['property_room_type'])\n",
    "\n",
    "\n",
    "roomtype_cat = pd.get_dummies(feature_train['property_room_type'])\n",
    "transformed_train = pd.concat([transformed_train,roomtype_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Oj8BhSP9iGHW",
    "outputId": "223a8e76-dd8b-4fb8-bec5-188f5ea20ddc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_amenities: count the number of amenities provided\n",
    "# remove nans by the mode \n",
    "\n",
    "# reviews_num: Power transform for skeweness \n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import skewtest\n",
    "\n",
    "\n",
    "feature_train['property_amenities'] =  feature_train['property_amenities'].str.split(', ').str.len()\n",
    "amenities_mode = feature_train['property_amenities'].mode()[0]\n",
    "feature_train['property_amenities'] = feature_train['property_amenities'].fillna(feature_train['property_amenities'].mode()[0])\n",
    "\n",
    "def powerTransform(df, column):\n",
    "  col  = np.array( df[column]).reshape(-1, 1)\n",
    "  pt = PowerTransformer(method='yeo-johnson', standardize=True,) \n",
    "  fit = pt.fit(col)\n",
    "  fit = pt.transform(col)\n",
    "  df[column] = fit\n",
    "  return df\n",
    "\n",
    "feature_train = powerTransform(feature_train,'property_amenities')\n",
    "feature_train = powerTransform(feature_train,'reviews_num')\n",
    "\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train.property_amenities],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,feature_train.reviews_num],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "-vxpyxsoS7aH",
    "outputId": "197f13ad-d9ba-4c70-e0ce-99e5ae6691e3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reviews_cleanliness\n",
    "# Good: above 5, Bad: below 5, None: average (could be set to \"Missing\", but people who find a property okay usually dont leave reviews, hence, average. Missing might be misleading)\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def reviewRecoding(df, column, new_column):\n",
    "  df[new_column] = df[column].fillna('Average')\n",
    "  df.loc[(df[column] <= 5) , new_column] = 'Bad'\n",
    "  df.loc[(df[column] > 5), new_column] = 'Good'\n",
    "  df = df.drop([column], axis = 1)\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_train = reviewRecoding(feature_train, 'reviews_cleanliness', 'reviews_cleanliness_n')\n",
    "feature_train = reviewRecoding(feature_train, 'reviews_checkin', 'reviews_checkin_n')\n",
    "feature_train = reviewRecoding(feature_train, 'reviews_location', 'reviews_communication_n')\n",
    "feature_train = reviewRecoding(feature_train, 'reviews_communication', 'reviews_communication_n')\n",
    "feature_train = reviewRecoding(feature_train, 'reviews_value', 'reviews_value_n')\n",
    "\n",
    "cleanliness_cat = pd.get_dummies(feature_train['reviews_cleanliness_n'])\n",
    "checkin_cat = pd.get_dummies(feature_train['reviews_checkin_n'])\n",
    "location_cat = pd.get_dummies(feature_train['reviews_communication_n'])\n",
    "communication_cat = pd.get_dummies(feature_train['reviews_communication_n'])\n",
    "value_cat = pd.get_dummies(feature_train['reviews_value_n'])\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,cleanliness_cat],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,checkin_cat],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,location_cat],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,communication_cat],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,value_cat],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "h44e1pGphnU-",
    "outputId": "c213aa80-c242-4cca-9bc6-6478fbcb577d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reviews_per_month (avg): right skewed - log transform\n",
    "#add 1 to all to avoid log problems\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "reviews_per_month_mode = feature_train['reviews_per_month'].mode()\n",
    "\n",
    "feature_train['reviews_per_month'] =feature_train['reviews_per_month'].fillna(feature_train['reviews_per_month'].mode())\n",
    "feature_train['reviews_per_month'] =feature_train['reviews_per_month']+1\n",
    "def reviewsLog(df, feature):\n",
    "  logTr = ColumnTransformer(transformers=[('lg', FunctionTransformer(np.log),[feature])])\n",
    "  log = logTr.fit_transform(df)\n",
    "  df[feature] = log\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_train = reviewsLog(feature_train, 'reviews_per_month')\n",
    "feature_train.reviews_per_month = feature_train.reviews_per_month.fillna(0)\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train.reviews_per_month],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uB-NrJX7CFtB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_scraped_at\n",
    "feature_train['property_scraped_at']= pd.to_datetime(feature_train['property_scraped_at'])\n",
    "feature_train['scraped_weekday']=feature_train['property_scraped_at'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encode property_scraped_at\n",
    "start_year = 2017\n",
    "days_since_2017 = (feature_train['property_scraped_at'] - pd.Timestamp(str(start_year))).dt.days\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,days_since_2017],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PTDekClz4lj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# property_last_updated\n",
    "# Naturally last_updated is a string :))))))\n",
    "\n",
    "def last_updated_conversion(update_var:str):\n",
    "  # 3 Buckets evenly distributed based on train, 2 for > 3 months, 0 for updated yesterday/today\n",
    "  if \"never\" in update_var:\n",
    "    return \"3 months or more\"\n",
    "  elif \"month\" in update_var:\n",
    "    if int(update_var.split(\" \")[0])>=3:\n",
    "      return \"3 months or more\"\n",
    "    else:\n",
    "      return \"1 week to 3 months\"\n",
    "  elif \"week\" in update_var:\n",
    "    return \"1 week to 3 months\"\n",
    "  else: # This catches yesterday etc. is actually equivalent to \"day\" in update_var\n",
    "    return \"Within days\"\n",
    "\n",
    "feature_train[\"property_last_updated_bucket\"] = [last_updated_conversion(x) for x in feature_train[\"property_last_updated\"]] \n",
    "feature_train = feature_train.drop([\"property_last_updated\"], axis = 1) # Dropping old columns\n",
    "\n",
    "last_updated_cat = pd.get_dummies(feature_train['property_last_updated_bucket'])\n",
    "transformed_train = pd.concat([transformed_train,last_updated_cat],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrrKDu1tEfGr"
   },
   "source": [
    "Decided to drop host_id because the main information we expected from this is contained in host_nr_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "S1nEcZj-z_5n",
    "outputId": "63d2d54e-e392-4676-b451-b0068271b546",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# host_since\n",
    "\n",
    "\n",
    "feature_train['host_since']=pd.to_datetime(feature_train['host_since'])\n",
    "host_since_mean = feature_train['host_since'].mean()\n",
    "feature_train['host_since'] = feature_train['host_since'].fillna(host_since_mean) # Simply imputing with mean, only had 1 NA anyways\n",
    "feature_train['host_since_scraped'] = feature_train['property_scraped_at']-feature_train['host_since']\n",
    "feature_train['host_since_scraped'] = feature_train['host_since_scraped'].dt.days.astype('int16')\n",
    "#feature_train['host_since_scraped'].hist()\n",
    "feature_train = feature_train.drop(['host_since'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['host_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "MiMepGGd0DS2",
    "outputId": "f2e4488c-ceec-490a-cff4-c6bbf437f35d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# host_response_time\n",
    "# Given 1k NAN decided to add extra missing category as never having received messages might be a signal, otherwise seems fine\n",
    "feature_train['host_response_time']=feature_train['host_response_time'].fillna(value=\"Missing\",inplace=True)\n",
    "#feature_train['host_response_time'].hist()\n",
    "\n",
    "host_response_cat = pd.get_dummies(feature_train['host_response_time'])\n",
    "transformed_train = pd.concat([transformed_train,host_response_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "IH9tg3AY0vxy",
    "outputId": "8b73cd29-2fc8-49eb-c684-90846b3ba60f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# host_response_rate\n",
    "# Impute 100% response rate for never having received a request which is \"fair\"\n",
    "feature_train['host_response_rate']=feature_train['host_response_rate'].fillna(value=100)\n",
    "# Similar to Rating skewness\n",
    "#feature_train['host_response_rate'].hist()\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['host_response_rate']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Xb0bIBdE0z3H",
    "outputId": "cb03eb02-cf11-4d95-d3de-24760bb533bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# host_nr_listings, host_nr_listings_total\n",
    "# Decided to drop host_nr_listings_total for now as it is basically the same information as host_nr_listings in the training set,\n",
    "# if one wants to squeeze more information out one could take the difference between the two as an extra feature\n",
    "# Basically everyone only has 1 property\n",
    "if \"host_nr_listings_total\" in feature_train.columns:\n",
    "  feature_train = feature_train.drop(['host_nr_listings_total'], axis = 1)\n",
    "\n",
    "feature_train.loc[(feature_train['host_nr_listings'] <= 1) , 'host_nr_listings_cat'] = 'One or less'\n",
    "feature_train.loc[(feature_train['host_nr_listings'] > 1) & (feature_train['host_nr_listings'] <=3), 'host_nr_listings_cat'] = 'Two to Three'\n",
    "feature_train.loc[(feature_train['host_nr_listings'] > 3), 'host_nr_listings_cat'] = 'More than 3'\n",
    "\n",
    "#feature_train['host_nr_listings_cat'].hist()\n",
    "feature_train = feature_train.drop(['host_nr_listings'], axis = 1) # Dropping old columns\n",
    "\n",
    "nr_listings_cat = pd.get_dummies(feature_train['host_nr_listings_cat'])\n",
    "transformed_train = pd.concat([transformed_train,nr_listings_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "88SZmOmK1Or6",
    "outputId": "ea70a231-ace0-418a-8f7a-1995b04de979",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# host_verified\n",
    "# csv sheet including everything that is \"verified\" e.g. email,phone reviews etc.\n",
    "# Same procedure as with property_summary\n",
    "feature_train['host_verified_count'] = feature_train['host_verified'].str.split(\",\").str.len()\n",
    "#feature_train['host_verified_count'].hist()\n",
    "feature_train = feature_train.drop(['host_verified'], axis = 1) # Dropping old columns\n",
    "\n",
    "feature_train['host_verified_count'] = feature_train['host_verified_count'].fillna(0)\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['host_verified_count']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "6qCdClt51SQC",
    "outputId": "2078a091-44d8-46a9-fcdf-11fb51d56937",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# booking_price_covers \n",
    "# Number of people that can live in the property for the price\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#feature_train['booking_price_covers'].hist()\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['booking_price_covers']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "_KRWkVxz1V7x",
    "outputId": "8c1e2a37-bf92-49c3-b424-49e799fa5bbe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# booking_min_nights\n",
    "# No NAs\n",
    "# Choosing to drop anything that requires booking more than 1 month which is only 37 observations. It should be noted that this will lead to us losing\n",
    "# some information on longterm rentals but I think this is preferable, in particular as these might be faulty observations anyways\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#for now: cap at 31\n",
    "feature_train.loc[feature_train[\"booking_max_nights\"] >31, \"booking_max_nights\"] = 31\n",
    "feature_train['booking_min_nights'].hist()\n",
    "transformed_train = pd.concat([transformed_train,feature_train['booking_min_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "f1iOG8mO1X0p",
    "outputId": "380059d1-e31b-4a52-e1ad-2967fea8bb89",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# booking_max_nights\n",
    "# no NAs\n",
    "# Capped everything above 2 months \n",
    "feature_train.loc[feature_train[\"booking_max_nights\"] >60, \"booking_max_nights\"] = 60\n",
    "#feature_train['booking_max_nights'].hist()\n",
    "transformed_train = pd.concat([transformed_train,feature_train['booking_max_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "xQgRX43N3T21",
    "outputId": "980688a0-c123-48b8-c3f5-a899e2e62b65",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# booking_cancel_policy\n",
    "# Seems fine\n",
    "#feature_train['booking_cancel_policy'].hist()\n",
    "\n",
    "cancel_cat = pd.get_dummies(feature_train['booking_cancel_policy'])\n",
    "transformed_train = pd.concat([transformed_train,cancel_cat],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALEFF731OV6K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# booking_availability_30/60/90/365\n",
    "# No NAs in any of them\n",
    "# Transform availabilities into representing the availability of respective time periods (e.g. 365-90 day availability = availability for last 9 months)\n",
    "# Only execute once\n",
    "\n",
    "feature_train['booking_availability_365'] = feature_train['booking_availability_365']-feature_train['booking_availability_90']\n",
    "feature_train['booking_availability_90'] = feature_train['booking_availability_90'] - feature_train['booking_availability_60']\n",
    "feature_train['booking_availability_60'] = feature_train['booking_availability_60']- feature_train['booking_availability_30']\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['booking_availability_30']],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,feature_train['booking_availability_60']],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,feature_train['booking_availability_90']],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,feature_train['booking_availability_365']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9Ih87cciK8k",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review_first, review_last\n",
    "# Could add difference between first and last, larger the better, could do some more sophisticated imputing. Chose simply the mode for now\n",
    "# Also skewed for now\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_train['reviews_first'] = pd.to_datetime(feature_train['reviews_first'])\n",
    "reviews_first_mode = feature_train['reviews_first'].mode()[0]\n",
    "feature_train['reviews_first'] = feature_train['reviews_first'].fillna(feature_train['reviews_first'].mode()[0])\n",
    "\n",
    "feature_train['reviews_last'] = pd.to_datetime(feature_train['reviews_last'])\n",
    "reviews_last_mode = feature_train['reviews_last'].mode()[0]\n",
    "feature_train['reviews_last'] = feature_train['reviews_last'].fillna(feature_train['reviews_last'].mode()[0])\n",
    "\n",
    "feature_train['reviews_first_since_scraped'] = (feature_train['property_scraped_at'] -feature_train['reviews_first']).dt.days.astype('int16')\n",
    "feature_train['reviews_last_since_scraped'] =  (feature_train['property_scraped_at'] -feature_train['reviews_last']).dt.days.astype('int16')\n",
    "\n",
    "feature_train = feature_train.drop(['reviews_first'], axis = 1) # Dropping old columns\n",
    "feature_train = feature_train.drop(['reviews_last'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['reviews_first_since_scraped']],axis=1)\n",
    "transformed_train = pd.concat([transformed_train,feature_train['reviews_last_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vS4DH7w4klS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reviews_rating\n",
    "reviews_rating_mode = feature_train['reviews_rating'].mode()[0]\n",
    "feature_train['reviews_rating']=feature_train['reviews_rating'].fillna(feature_train['reviews_rating'].mode()[0])\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['reviews_rating']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-te7JzlM5NqY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reviews_acc\n",
    "reviews_acc_mode = feature_train['reviews_acc'].mode()[0]\n",
    "feature_train['reviews_acc']=feature_train['reviews_acc'].fillna(feature_train['reviews_acc'].mode()[0])\n",
    "\n",
    "transformed_train = pd.concat([transformed_train,feature_train['reviews_acc']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fix column names\n",
    "transformed_train.columns = transformed_train.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nans = transformed_train.isna().any()\n",
    "print(nans.loc[nans==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Avk9qpQ8gylH",
    "outputId": "7c775fdf-c082-4ca1-bb02-8693ff3741de"
   },
   "outputs": [],
   "source": [
    "eda=AV.AutoViz(filename=\"\",dfte=feature_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creation of Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping this would be the same as the word doc table\n",
    "feature_test = feature_test.drop(['property_id','property_space','property_desc','property_neighborhood','property_notes','property_access',\n",
    "                                    \"property_interaction\",\"property_rules\",\"host_location\",\"host_about\",\"host_id\",\"property_sqfeet\"], axis = 1)\n",
    "feature_test = feature_test.drop([\"property_transit\"],axis=1)\n",
    "transformed_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All df to lower case -works\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def toLower(df):\n",
    "    df = df.apply(lambda x: x.str.lower() if x.dtype=='object' else x)\n",
    "    return df\n",
    "\n",
    "toLower = FunctionTransformer(toLower)\n",
    "toLower.fit_transform(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property name: replaced by the word count & new column:missing? - works\n",
    "\n",
    "feature_test['property_name_miss'] = np.where(feature_test['property_name'].isna(), 1, 0)\n",
    "# Should refactor this because if run twice it throws an error\n",
    "feature_test.property_name = feature_test['property_name'].str.split().str.len()\n",
    "feature_test.property_name = feature_test.property_name.fillna(0)\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test.property_name],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,feature_test.property_name_miss],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property summary: new columns - summary_missing & property_summary_count\n",
    "## Missing = 'nan' or one word in summary\n",
    "str_df  = pd.DataFrame()\n",
    "str_df['condition'] = feature_test['property_summary'].str.match(r'\\A[\\w-]+\\Z')\n",
    "\n",
    "feature_test = pd.merge(str_df, feature_test, left_index=True, right_index=True)\n",
    "feature_test['property_summary_miss'] = np.where(feature_test['condition'].isna(), 1, 0)\n",
    "feature_test = feature_test.drop(['condition'], axis = 1)\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test.property_summary_miss],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Property_summary_count: remove numbers, punctuation \n",
    "feature_test['property_summary'] = feature_test['property_summary'].str.replace('\\d+', '')\n",
    "feature_test['property_summary'] = feature_test['property_summary'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "feature_test['property_summary_count'] = feature_test['property_summary'].str.split().str.len()\n",
    "feature_test = feature_test.drop(['property_summary'], axis = 1)\n",
    "feature_test.property_summary_count = feature_test.property_summary_count.fillna(0)\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test.property_summary_count],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_zipcode: replaced with mode\n",
    "\n",
    "feature_test['property_zipcode'] = feature_test['property_zipcode'] .fillna(zipcode_mode)\n",
    "\n",
    "feature_test.loc[feature_test['property_zipcode']=='11 20','property_zipcode']=1120\n",
    "\n",
    "freq_table=feature_test['property_zipcode'].value_counts()\n",
    "\n",
    "#encoding: preserve 11 most frequent categories\n",
    "feature_test.property_zipcode=feature_test.property_zipcode.astype(int)\n",
    "zipcode_cats = pd.get_dummies(feature_test.property_zipcode)\n",
    "sorted_zipcode_cats = zipcode_cats[zipcode_cats.sum().sort_values(ascending=False).index]\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,sorted_zipcode_cats.iloc[:,:10]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_type: regrouped into - apartment, house, other\n",
    "threshold_percent = 3\n",
    "\n",
    "series = pd.value_counts(feature_test['property_type'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)\n",
    "\n",
    "feature_test = feature_test.assign(property_type = np.where(feature_test['property_type'].isin(series[mask].index),'Other', feature_test['property_type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode property_type\n",
    "type_cats = pd.get_dummies(feature_test['property_type'])\n",
    "transformed_test = pd.concat([transformed_test,type_cats],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bathrooms: regroup into 1. None 2. One 3. More than one\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_test['property_bathrooms'] =feature_test['property_bathrooms'].fillna(bathrooms_mode)\n",
    "\n",
    "np.unique(feature_test['property_bathrooms'],return_counts=True)\n",
    "\n",
    "feature_test['property_bathrooms_cat']= 'One'\n",
    "feature_test.loc[(feature_test['property_bathrooms'] < 1) , 'property_bathrooms_cat'] = 'None'\n",
    "feature_test.loc[(feature_test['property_bathrooms'] > 1), 'property_bathrooms_cat'] = 'More than one'\n",
    "\n",
    "feature_test = feature_test.drop(['property_bathrooms'], axis = 1)\n",
    "feature_test['property_bathrooms_cat']\n",
    "\n",
    "bathroom_cats = pd.get_dummies(feature_test['property_bathrooms_cat'])\n",
    "transformed_test = pd.concat([transformed_test,bathroom_cats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bedrooms\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def NumToCategory(df, column,new_column, missing_value = 0):\n",
    "  df[column] =df[column].fillna(missing_value)\n",
    "\n",
    "  df.loc[(df[column] == 1), new_column] = 'One'\n",
    "  df.loc[(df[column] == 2), new_column] = 'Two'\n",
    "  df.loc[(df[column] == 3), new_column] = 'Three'\n",
    "  df.loc[(df[column] > 3), new_column] = 'More than three'\n",
    "\n",
    "  df = df.drop([column], axis = 1)\n",
    "  return df\n",
    "\n",
    "feature_test = NumToCategory(feature_test, 'property_beds', 'property_beds_cat')\n",
    "feature_test = NumToCategory(feature_test, 'property_bedrooms', 'property_bedrooms_cat')\n",
    "\n",
    "beds_dummies = pd.get_dummies(feature_test['property_beds_cat'])\n",
    "bedrooms_dummies = pd.get_dummies(feature_test['property_bedrooms_cat'])\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,beds_dummies,bedrooms_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bed_type: ok\n",
    "feature_test['property_bed_type'].unique()\n",
    "# room type: ok\n",
    "pd.value_counts(feature_test['property_room_type'])\n",
    "\n",
    "\n",
    "roomtype_cat = pd.get_dummies(feature_test['property_room_type'])\n",
    "transformed_test = pd.concat([transformed_test,roomtype_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_amenities: count the number of amenities provided\n",
    "# remove nans by the mode \n",
    "\n",
    "# reviews_num: Power transform for skeweness \n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import skewtest\n",
    "\n",
    "feature_test['property_amenities'] =  feature_test['property_amenities'].str.split(', ').str.len()\n",
    "feature_test['property_amenities'] = feature_test['property_amenities'].fillna(amenities_mode)\n",
    "\n",
    "def powerTransform(df, column):\n",
    "  col  = np.array( df[column]).reshape(-1, 1)\n",
    "  pt = PowerTransformer(method='yeo-johnson', standardize=True,) \n",
    "  fit = pt.fit(col)\n",
    "  fit = pt.transform(col)\n",
    "  df[column] = fit\n",
    "  return df\n",
    "\n",
    "feature_test = powerTransform(feature_test,'property_amenities')\n",
    "feature_test = powerTransform(feature_test,'reviews_num')\n",
    "\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test.property_amenities],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,feature_test.reviews_num],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_cleanliness\n",
    "# Good: above 5, Bad: below 5, None: average (could be set to \"Missing\", but people who find a property okay usually dont leave reviews, hence, average. Missing might be misleading)\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def reviewRecoding(df, column, new_column):\n",
    "  df[new_column] = df[column].fillna('Average')\n",
    "  df.loc[(df[column] <= 5) , new_column] = 'Bad'\n",
    "  df.loc[(df[column] > 5), new_column] = 'Good'\n",
    "  df = df.drop([column], axis = 1)\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_test = reviewRecoding(feature_test, 'reviews_cleanliness', 'reviews_cleanliness_n')\n",
    "feature_test = reviewRecoding(feature_test, 'reviews_checkin', 'reviews_checkin_n')\n",
    "feature_test = reviewRecoding(feature_test, 'reviews_location', 'reviews_communication_n')\n",
    "feature_test = reviewRecoding(feature_test, 'reviews_communication', 'reviews_communication_n')\n",
    "feature_test = reviewRecoding(feature_test, 'reviews_value', 'reviews_value_n')\n",
    "\n",
    "cleanliness_cat = pd.get_dummies(feature_test['reviews_cleanliness_n'])\n",
    "checkin_cat = pd.get_dummies(feature_test['reviews_checkin_n'])\n",
    "location_cat = pd.get_dummies(feature_test['reviews_communication_n'])\n",
    "communication_cat = pd.get_dummies(feature_test['reviews_communication_n'])\n",
    "value_cat = pd.get_dummies(feature_test['reviews_value_n'])\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,cleanliness_cat],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,checkin_cat],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,location_cat],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,communication_cat],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,value_cat],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_per_month (avg): right skewed - log transform\n",
    "#add 1 to all to avoid log problems\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "feature_test['reviews_per_month'] =feature_test['reviews_per_month'].fillna(reviews_per_month_mode)\n",
    "feature_test['reviews_per_month'] =feature_test['reviews_per_month']+1\n",
    "def reviewsLog(df, feature):\n",
    "  logTr = ColumnTransformer(transformers=[('lg', FunctionTransformer(np.log),[feature])])\n",
    "  log = logTr.fit_transform(df)\n",
    "  df[feature] = log\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_test = reviewsLog(feature_test, 'reviews_per_month')\n",
    "feature_test.reviews_per_month = feature_test.reviews_per_month.fillna(0)\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test.reviews_per_month],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_scraped_at\n",
    "feature_test['property_scraped_at']= pd.to_datetime(feature_test['property_scraped_at'])\n",
    "feature_test['scraped_weekday']=feature_test['property_scraped_at'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode property_scraped_at\n",
    "start_year = 2017\n",
    "days_since_2017 = (feature_test['property_scraped_at'] - pd.Timestamp(str(start_year))).dt.days\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,days_since_2017],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_last_updated\n",
    "# Naturally last_updated is a string :))))))\n",
    "\n",
    "def last_updated_conversion(update_var:str):\n",
    "  # 3 Buckets evenly distributed based on test, 2 for > 3 months, 0 for updated yesterday/today\n",
    "  if \"never\" in update_var:\n",
    "    return \"3 months or more\"\n",
    "  elif \"month\" in update_var:\n",
    "    if int(update_var.split(\" \")[0])>=3:\n",
    "      return \"3 months or more\"\n",
    "    else:\n",
    "      return \"1 week to 3 months\"\n",
    "  elif \"week\" in update_var:\n",
    "    return \"1 week to 3 months\"\n",
    "  else: # This catches yesterday etc. is actually equivalent to \"day\" in update_var\n",
    "    return \"Within days\"\n",
    "\n",
    "feature_test[\"property_last_updated_bucket\"] = [last_updated_conversion(x) for x in feature_test[\"property_last_updated\"]] \n",
    "feature_test = feature_test.drop([\"property_last_updated\"], axis = 1) # Dropping old columns\n",
    "\n",
    "last_updated_cat = pd.get_dummies(feature_test['property_last_updated_bucket'])\n",
    "transformed_test = pd.concat([transformed_test,last_updated_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_since\n",
    "feature_test['host_since']=pd.to_datetime(feature_test['host_since'])\n",
    "feature_test['host_since'] = feature_test['host_since'].fillna(host_since_mean) # Simply imputing with mean, only had 1 NA anyways\n",
    "feature_test['host_since_scraped'] = feature_test['property_scraped_at']-feature_test['host_since']\n",
    "feature_test['host_since_scraped'] = feature_test['host_since_scraped'].dt.days.astype('int16')\n",
    "#feature_test['host_since_scraped'].hist()\n",
    "feature_test = feature_test.drop(['host_since'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['host_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_response_time\n",
    "# Given 1k NAN decided to add extra missing category as never having received messages might be a signal, otherwise seems fine\n",
    "feature_test['host_response_time']=feature_test['host_response_time'].fillna(value=\"Missing\",inplace=True)\n",
    "#feature_test['host_response_time'].hist()\n",
    "\n",
    "host_response_cat = pd.get_dummies(feature_test['host_response_time'])\n",
    "transformed_test = pd.concat([transformed_test,host_response_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_response_rate\n",
    "# Impute 100% response rate for never having received a request which is \"fair\"\n",
    "feature_test['host_response_rate']=feature_test['host_response_rate'].fillna(value=100)\n",
    "# Similar to Rating skewness\n",
    "#feature_test['host_response_rate'].hist()\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['host_response_rate']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_nr_listings, host_nr_listings_total\n",
    "# Decided to drop host_nr_listings_total for now as it is basically the same information as host_nr_listings in the testing set,\n",
    "# if one wants to squeeze more information out one could take the difference between the two as an extra feature\n",
    "# Basically everyone only has 1 property\n",
    "if \"host_nr_listings_total\" in feature_test.columns:\n",
    "  feature_test = feature_test.drop(['host_nr_listings_total'], axis = 1)\n",
    "\n",
    "feature_test.loc[(feature_test['host_nr_listings'] <= 1) , 'host_nr_listings_cat'] = 'One or less'\n",
    "feature_test.loc[(feature_test['host_nr_listings'] > 1) & (feature_test['host_nr_listings'] <=3), 'host_nr_listings_cat'] = 'Two to Three'\n",
    "feature_test.loc[(feature_test['host_nr_listings'] > 3), 'host_nr_listings_cat'] = 'More than 3'\n",
    "\n",
    "#feature_test['host_nr_listings_cat'].hist()\n",
    "feature_test = feature_test.drop(['host_nr_listings'], axis = 1) # Dropping old columns\n",
    "\n",
    "nr_listings_cat = pd.get_dummies(feature_test['host_nr_listings_cat'])\n",
    "transformed_test = pd.concat([transformed_test,nr_listings_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_verified\n",
    "# csv sheet including everything that is \"verified\" e.g. email,phone reviews etc.\n",
    "# Same procedure as with property_summary\n",
    "feature_test['host_verified_count'] = feature_test['host_verified'].str.split(\",\").str.len()\n",
    "#feature_test['host_verified_count'].hist()\n",
    "feature_test = feature_test.drop(['host_verified'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['host_verified_count']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_price_covers \n",
    "# Number of people that can live in the property for the price\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#feature_test['booking_price_covers'].hist()\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['booking_price_covers']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_min_nights\n",
    "# No NAs\n",
    "# Choosing to drop anything that requires booking more than 1 month which is only 37 observations. It should be noted that this will lead to us losing\n",
    "# some information on longterm rentals but I think this is preferable, in particular as these might be faulty observations anyways\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#for now: cap at 31\n",
    "feature_test.loc[feature_test[\"booking_max_nights\"] >31, \"booking_max_nights\"] = 31\n",
    "feature_test['booking_min_nights'].hist()\n",
    "transformed_test = pd.concat([transformed_test,feature_test['booking_min_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_max_nights\n",
    "# no NAs\n",
    "# Capped everything above 2 months \n",
    "feature_test.loc[feature_test[\"booking_max_nights\"] >60, \"booking_max_nights\"] = 60\n",
    "#feature_test['booking_max_nights'].hist()\n",
    "transformed_test = pd.concat([transformed_test,feature_test['booking_max_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_cancel_policy\n",
    "# Seems fine\n",
    "#feature_test['booking_cancel_policy'].hist()\n",
    "\n",
    "cancel_cat = pd.get_dummies(feature_test['booking_cancel_policy'])\n",
    "transformed_test = pd.concat([transformed_test,cancel_cat],axis=1)\n",
    "transformed_test['super_strict_30']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_availability_30/60/90/365\n",
    "# No NAs in any of them\n",
    "# Transform availabilities into representing the availability of respective time periods (e.g. 365-90 day availability = availability for last 9 months)\n",
    "# Only execute once\n",
    "\n",
    "feature_test['booking_availability_365'] = feature_test['booking_availability_365']-feature_test['booking_availability_90']\n",
    "feature_test['booking_availability_90'] = feature_test['booking_availability_90'] - feature_test['booking_availability_60']\n",
    "feature_test['booking_availability_60'] = feature_test['booking_availability_60']- feature_test['booking_availability_30']\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['booking_availability_30']],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,feature_test['booking_availability_60']],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,feature_test['booking_availability_90']],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,feature_test['booking_availability_365']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_first, review_last\n",
    "# Could add difference between first and last, larger the better, could do some more sophisticated imputing. Chose simply the mode for now\n",
    "# Also skewed for now\n",
    "feature_test['reviews_first'] = pd.to_datetime(feature_test['reviews_first'])\n",
    "feature_test['reviews_first'] = feature_test['reviews_first'].fillna(reviews_first_mode)\n",
    "\n",
    "feature_test['reviews_last'] = pd.to_datetime(feature_test['reviews_last'])\n",
    "feature_test['reviews_last'] = feature_test['reviews_last'].fillna(reviews_last_mode)\n",
    "\n",
    "feature_test['reviews_first_since_scraped'] = (feature_test['property_scraped_at'] -feature_test['reviews_first']).dt.days.astype('int16')\n",
    "feature_test['reviews_last_since_scraped'] =  (feature_test['property_scraped_at'] -feature_test['reviews_last']).dt.days.astype('int16')\n",
    "\n",
    "feature_test = feature_test.drop(['reviews_first'], axis = 1) # Dropping old columns\n",
    "feature_test = feature_test.drop(['reviews_last'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['reviews_first_since_scraped']],axis=1)\n",
    "transformed_test = pd.concat([transformed_test,feature_test['reviews_last_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_rating\n",
    "feature_test['reviews_rating']=feature_test['reviews_rating'].fillna(reviews_rating_mode)\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['reviews_rating']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_acc\n",
    "feature_test['reviews_acc']=feature_test['reviews_acc'].fillna(reviews_acc_mode)\n",
    "\n",
    "transformed_test = pd.concat([transformed_test,feature_test['reviews_acc']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix column names\n",
    "transformed_test.columns = transformed_test.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = transformed_test.isna().any()\n",
    "print(nans.loc[nans==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(transformed_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(transformed_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(transformed_train)\n",
    "\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca4 = PCA(n_components=4)\n",
    "pca4.fit(transformed_train)\n",
    "\n",
    "train_PCA = pca4.transform(transformed_train)\n",
    "test_PCA = pca4.transform(transformed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create models\n",
    "lr = LinearRegression()\n",
    "\n",
    "#fit the models\n",
    "lr.fit(train_PCA,target_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred = lr.predict(test_PCA)\n",
    "\n",
    "mse = mean_squared_error(y_pred,target_test,squared=False)\n",
    "\n",
    "print(\"Mean Squared Error lr: {:.2f}\".format(mse))\n",
    "\n",
    "# Print the model coefficients\n",
    "#print(\"Model coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Q94tMenIaC2x",
    "outputId": "92dc5f72-5c9d-4a65-e897-b0f8145b759c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create models\n",
    "lasso_low = Lasso(alpha=0.5)\n",
    "lasso_mid = Lasso(alpha=1)\n",
    "lasso_large = Lasso(alpha=100)\n",
    "\n",
    "#fit the models\n",
    "lasso_low.fit(transformed_train,target_train)\n",
    "lasso_mid.fit(transformed_train,target_train)\n",
    "lasso_large.fit(transformed_train,target_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_low = lasso_low.predict(transformed_test)\n",
    "y_pred_mid = lasso_mid.predict(transformed_test)\n",
    "y_pred_large = lasso_large.predict(transformed_test)\n",
    "\n",
    "mse_low = mean_squared_error(y_pred_low,target_test,squared=False)\n",
    "mse_mid = mean_squared_error(y_pred_mid,target_test,squared=False)\n",
    "mse_large = mean_squared_error(y_pred_large,target_test,squared=False)\n",
    "\n",
    "print(\"Mean Squared Error lowlasso: {:.2f}\".format(mse_low))\n",
    "print(\"Mean Squared Error midlasso: {:.2f}\".format(mse_mid))\n",
    "print(\"Mean Squared Error largelasso: {:.2f}\".format(mse_large))\n",
    "\n",
    "# Print the model coefficients\n",
    "#print(\"Model coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "# Create models\n",
    "poisson = PoissonRegressor()\n",
    "\n",
    "#fit the models\n",
    "poisson.fit(train_PCA,target_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred = lr.predict(test_PCA)\n",
    "\n",
    "mse = mean_squared_error(y_pred,target_test,squared=False)\n",
    "\n",
    "#print(\"Mean Squared Error lr: {:.2f}\".format(mse))\n",
    "\n",
    "# Print the model coefficients\n",
    "#print(\"Model coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network: multi-layer-perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Create models\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(75,50),random_state=2023,max_iter=10000)\n",
    "\n",
    "#fit the models\n",
    "mlp.fit(transformed_train,target_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred = mlp.predict(transformed_test)\n",
    "\n",
    "mse = mean_squared_error(y_pred,target_test,squared=False)\n",
    "\n",
    "print(\"Mean Squared Error mlp: {:.2f}\".format(mse))\n",
    "\n",
    "# Print the model coefficients\n",
    "#print(\"Model coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fit Model on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping this would be the same as the word doc table\n",
    "feature_full = feature_full.drop(['property_id','property_space','property_desc','property_neighborhood','property_notes','property_access',\n",
    "                                    \"property_interaction\",\"property_rules\",\"host_location\",\"host_about\",\"host_id\",\"property_sqfeet\"], axis = 1)\n",
    "feature_full = feature_full.drop([\"property_transit\"],axis=1)\n",
    "transformed_full = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All df to lower case -works\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def toLower(df):\n",
    "    df = df.apply(lambda x: x.str.lower() if x.dtype=='object' else x)\n",
    "    return df\n",
    "\n",
    "toLower = FunctionTransformer(toLower)\n",
    "toLower.fit_transform(feature_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property name: replaced by the word count & new column:missing? - works\n",
    "\n",
    "feature_full['property_name_miss'] = np.where(feature_full['property_name'].isna(), 1, 0)\n",
    "# Should refactor this because if run twice it throws an error\n",
    "feature_full.property_name = feature_full['property_name'].str.split().str.len()\n",
    "feature_full.property_name = feature_full.property_name.fillna(0)\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full.property_name],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,feature_full.property_name_miss],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property summary: new columns - summary_missing & property_summary_count\n",
    "## Missing = 'nan' or one word in summary\n",
    "str_df  = pd.DataFrame()\n",
    "str_df['condition'] = feature_full['property_summary'].str.match(r'\\A[\\w-]+\\Z')\n",
    "\n",
    "feature_full = pd.merge(str_df, feature_full, left_index=True, right_index=True)\n",
    "feature_full['property_summary_miss'] = np.where(feature_full['condition'].isna(), 1, 0)\n",
    "feature_full = feature_full.drop(['condition'], axis = 1)\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full.property_summary_miss],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Property_summary_count: remove numbers, punctuation \n",
    "feature_full['property_summary'] = feature_full['property_summary'].str.replace('\\d+', '')\n",
    "feature_full['property_summary'] = feature_full['property_summary'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "feature_full['property_summary_count'] = feature_full['property_summary'].str.split().str.len()\n",
    "feature_full = feature_full.drop(['property_summary'], axis = 1)\n",
    "feature_full.property_summary_count = feature_full.property_summary_count.fillna(0)\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full.property_summary_count],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_zipcode: replaced with mode\n",
    "zipcode_mode=feature_full['property_zipcode'].mode()[0]\n",
    "feature_full['property_zipcode'] = feature_full['property_zipcode'] .fillna(feature_full['property_zipcode'].mode()[0])\n",
    "\n",
    "feature_full.loc[feature_full['property_zipcode']=='11 20','property_zipcode']=1120\n",
    "\n",
    "freq_table=feature_full['property_zipcode'].value_counts()\n",
    "\n",
    "#encoding: preserve 11 most frequent categories\n",
    "feature_full.property_zipcode=feature_full.property_zipcode.astype(int)\n",
    "zipcode_cats = pd.get_dummies(feature_full.property_zipcode)\n",
    "sorted_zipcode_cats = zipcode_cats[zipcode_cats.sum().sort_values(ascending=False).index]\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,sorted_zipcode_cats.iloc[:,:10]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_type: regrouped into - apartment, house, other\n",
    "threshold_percent = 3\n",
    "\n",
    "series = pd.value_counts(feature_full['property_type'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)\n",
    "\n",
    "feature_full = feature_full.assign(property_type = np.where(feature_full['property_type'].isin(series[mask].index),'Other', feature_full['property_type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode property_type\n",
    "type_cats = pd.get_dummies(feature_full['property_type'])\n",
    "transformed_full = pd.concat([transformed_full,type_cats],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bathrooms: regroup into 1. None 2. One 3. More than one\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bathrooms_mode = feature_full['property_bathrooms'].mode()\n",
    "feature_full['property_bathrooms'] =feature_full['property_bathrooms'].fillna(feature_full['property_bathrooms'].mode())\n",
    "\n",
    "np.unique(feature_full['property_bathrooms'],return_counts=True)\n",
    "\n",
    "feature_full['property_bathrooms_cat']= 'One'\n",
    "feature_full.loc[(feature_full['property_bathrooms'] < 1) , 'property_bathrooms_cat'] = 'None'\n",
    "feature_full.loc[(feature_full['property_bathrooms'] > 1), 'property_bathrooms_cat'] = 'More than one'\n",
    "\n",
    "feature_full = feature_full.drop(['property_bathrooms'], axis = 1)\n",
    "feature_full['property_bathrooms_cat']\n",
    "\n",
    "bathroom_cats = pd.get_dummies(feature_full['property_bathrooms_cat'])\n",
    "transformed_full = pd.concat([transformed_full,bathroom_cats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bedrooms\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def NumToCategory(df, column,new_column, missing_value = 0):\n",
    "  df[column] =df[column].fillna(missing_value)\n",
    "\n",
    "  df.loc[(df[column] == 1), new_column] = 'One'\n",
    "  df.loc[(df[column] == 2), new_column] = 'Two'\n",
    "  df.loc[(df[column] == 3), new_column] = 'Three'\n",
    "  df.loc[(df[column] > 3), new_column] = 'More than three'\n",
    "\n",
    "  df = df.drop([column], axis = 1)\n",
    "  return df\n",
    "\n",
    "feature_full = NumToCategory(feature_full, 'property_beds', 'property_beds_cat')\n",
    "feature_full = NumToCategory(feature_full, 'property_bedrooms', 'property_bedrooms_cat')\n",
    "\n",
    "beds_dummies = pd.get_dummies(feature_full['property_beds_cat'])\n",
    "bedrooms_dummies = pd.get_dummies(feature_full['property_bedrooms_cat'])\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,beds_dummies,bedrooms_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bed_type: ok\n",
    "feature_full['property_bed_type'].unique()\n",
    "# room type: ok\n",
    "pd.value_counts(feature_full['property_room_type'])\n",
    "\n",
    "\n",
    "roomtype_cat = pd.get_dummies(feature_full['property_room_type'])\n",
    "transformed_full = pd.concat([transformed_full,roomtype_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_amenities: count the number of amenities provided\n",
    "# remove nans by the mode \n",
    "\n",
    "# reviews_num: Power transform for skeweness \n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import skewtest\n",
    "\n",
    "\n",
    "feature_full['property_amenities'] =  feature_full['property_amenities'].str.split(', ').str.len()\n",
    "amenities_mode = feature_full['property_amenities'].mode()[0]\n",
    "feature_full['property_amenities'] = feature_full['property_amenities'].fillna(feature_full['property_amenities'].mode()[0])\n",
    "\n",
    "def powerTransform(df, column):\n",
    "  col  = np.array( df[column]).reshape(-1, 1)\n",
    "  pt = PowerTransformer(method='yeo-johnson', standardize=True,) \n",
    "  fit = pt.fit(col)\n",
    "  fit = pt.transform(col)\n",
    "  df[column] = fit\n",
    "  return df\n",
    "\n",
    "feature_full = powerTransform(feature_full,'property_amenities')\n",
    "feature_full = powerTransform(feature_full,'reviews_num')\n",
    "\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full.property_amenities],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,feature_full.reviews_num],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_cleanliness\n",
    "# Good: above 5, Bad: below 5, None: average (could be set to \"Missing\", but people who find a property okay usually dont leave reviews, hence, average. Missing might be misleading)\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def reviewRecoding(df, column, new_column):\n",
    "  df[new_column] = df[column].fillna('Average')\n",
    "  df.loc[(df[column] <= 5) , new_column] = 'Bad'\n",
    "  df.loc[(df[column] > 5), new_column] = 'Good'\n",
    "  df = df.drop([column], axis = 1)\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_full = reviewRecoding(feature_full, 'reviews_cleanliness', 'reviews_cleanliness_n')\n",
    "feature_full = reviewRecoding(feature_full, 'reviews_checkin', 'reviews_checkin_n')\n",
    "feature_full = reviewRecoding(feature_full, 'reviews_location', 'reviews_communication_n')\n",
    "feature_full = reviewRecoding(feature_full, 'reviews_communication', 'reviews_communication_n')\n",
    "feature_full = reviewRecoding(feature_full, 'reviews_value', 'reviews_value_n')\n",
    "\n",
    "cleanliness_cat = pd.get_dummies(feature_full['reviews_cleanliness_n'])\n",
    "checkin_cat = pd.get_dummies(feature_full['reviews_checkin_n'])\n",
    "location_cat = pd.get_dummies(feature_full['reviews_communication_n'])\n",
    "communication_cat = pd.get_dummies(feature_full['reviews_communication_n'])\n",
    "value_cat = pd.get_dummies(feature_full['reviews_value_n'])\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,cleanliness_cat],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,checkin_cat],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,location_cat],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,communication_cat],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,value_cat],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_per_month (avg): right skewed - log transform\n",
    "#add 1 to all to avoid log problems\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "reviews_per_month_mode = feature_full['reviews_per_month'].mode()\n",
    "\n",
    "feature_full['reviews_per_month'] =feature_full['reviews_per_month'].fillna(feature_full['reviews_per_month'].mode())\n",
    "feature_full['reviews_per_month'] =feature_full['reviews_per_month']+1\n",
    "def reviewsLog(df, feature):\n",
    "  logTr = ColumnTransformer(transformers=[('lg', FunctionTransformer(np.log),[feature])])\n",
    "  log = logTr.fit_transform(df)\n",
    "  df[feature] = log\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_full = reviewsLog(feature_full, 'reviews_per_month')\n",
    "feature_full.reviews_per_month = feature_full.reviews_per_month.fillna(0)\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full.reviews_per_month],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_scraped_at\n",
    "feature_full['property_scraped_at']= pd.to_datetime(feature_full['property_scraped_at'])\n",
    "feature_full['scraped_weekday']=feature_full['property_scraped_at'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode property_scraped_at\n",
    "start_year = 2017\n",
    "days_since_2017 = (feature_full['property_scraped_at'] - pd.Timestamp(str(start_year))).dt.days\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,days_since_2017],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_last_updated\n",
    "# Naturally last_updated is a string :))))))\n",
    "\n",
    "def last_updated_conversion(update_var:str):\n",
    "  # 3 Buckets evenly distributed based on full, 2 for > 3 months, 0 for updated yesterday/today\n",
    "  if \"never\" in update_var:\n",
    "    return \"3 months or more\"\n",
    "  elif \"month\" in update_var:\n",
    "    if int(update_var.split(\" \")[0])>=3:\n",
    "      return \"3 months or more\"\n",
    "    else:\n",
    "      return \"1 week to 3 months\"\n",
    "  elif \"week\" in update_var:\n",
    "    return \"1 week to 3 months\"\n",
    "  else: # This catches yesterday etc. is actually equivalent to \"day\" in update_var\n",
    "    return \"Within days\"\n",
    "\n",
    "feature_full[\"property_last_updated_bucket\"] = [last_updated_conversion(x) for x in feature_full[\"property_last_updated\"]] \n",
    "feature_full = feature_full.drop([\"property_last_updated\"], axis = 1) # Dropping old columns\n",
    "\n",
    "last_updated_cat = pd.get_dummies(feature_full['property_last_updated_bucket'])\n",
    "transformed_full = pd.concat([transformed_full,last_updated_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_since\n",
    "feature_full['host_since']=pd.to_datetime(feature_full['host_since'])\n",
    "host_since_mean = feature_full['host_since'].mean()\n",
    "feature_full['host_since'] = feature_full['host_since'].fillna(host_since_mean) # Simply imputing with mean, only had 1 NA anyways\n",
    "feature_full['host_since_scraped'] = feature_full['property_scraped_at']-feature_full['host_since']\n",
    "feature_full['host_since_scraped'] = feature_full['host_since_scraped'].dt.days.astype('int16')\n",
    "#feature_full['host_since_scraped'].hist()\n",
    "feature_full = feature_full.drop(['host_since'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['host_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_response_time\n",
    "# Given 1k NAN decided to add extra missing category as never having received messages might be a signal, otherwise seems fine\n",
    "feature_full['host_response_time']=feature_full['host_response_time'].fillna(value=\"Missing\",inplace=True)\n",
    "#feature_full['host_response_time'].hist()\n",
    "\n",
    "host_response_cat = pd.get_dummies(feature_full['host_response_time'])\n",
    "transformed_full = pd.concat([transformed_full,host_response_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_response_rate\n",
    "# Impute 100% response rate for never having received a request which is \"fair\"\n",
    "feature_full['host_response_rate']=feature_full['host_response_rate'].fillna(value=100)\n",
    "# Similar to Rating skewness\n",
    "#feature_full['host_response_rate'].hist()\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['host_response_rate']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_nr_listings, host_nr_listings_total\n",
    "# Decided to drop host_nr_listings_total for now as it is basically the same information as host_nr_listings in the fulling set,\n",
    "# if one wants to squeeze more information out one could take the difference between the two as an extra feature\n",
    "# Basically everyone only has 1 property\n",
    "if \"host_nr_listings_total\" in feature_full.columns:\n",
    "  feature_full = feature_full.drop(['host_nr_listings_total'], axis = 1)\n",
    "\n",
    "feature_full.loc[(feature_full['host_nr_listings'] <= 1) , 'host_nr_listings_cat'] = 'One or less'\n",
    "feature_full.loc[(feature_full['host_nr_listings'] > 1) & (feature_full['host_nr_listings'] <=3), 'host_nr_listings_cat'] = 'Two to Three'\n",
    "feature_full.loc[(feature_full['host_nr_listings'] > 3), 'host_nr_listings_cat'] = 'More than 3'\n",
    "\n",
    "#feature_full['host_nr_listings_cat'].hist()\n",
    "feature_full = feature_full.drop(['host_nr_listings'], axis = 1) # Dropping old columns\n",
    "\n",
    "nr_listings_cat = pd.get_dummies(feature_full['host_nr_listings_cat'])\n",
    "transformed_full = pd.concat([transformed_full,nr_listings_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_verified\n",
    "# csv sheet including everything that is \"verified\" e.g. email,phone reviews etc.\n",
    "# Same procedure as with property_summary\n",
    "feature_full['host_verified_count'] = feature_full['host_verified'].str.split(\",\").str.len()\n",
    "#feature_full['host_verified_count'].hist()\n",
    "feature_full = feature_full.drop(['host_verified'], axis = 1) # Dropping old columns\n",
    "\n",
    "feature_full['host_verified_count'] = feature_full['host_verified_count'].fillna(0)\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['host_verified_count']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_price_covers \n",
    "# Number of people that can live in the property for the price\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#feature_full['booking_price_covers'].hist()\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['booking_price_covers']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_min_nights\n",
    "# No NAs\n",
    "# Choosing to drop anything that requires booking more than 1 month which is only 37 observations. It should be noted that this will lead to us losing\n",
    "# some information on longterm rentals but I think this is preferable, in particular as these might be faulty observations anyways\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#for now: cap at 31\n",
    "feature_full.loc[feature_full[\"booking_max_nights\"] >31, \"booking_max_nights\"] = 31\n",
    "feature_full['booking_min_nights'].hist()\n",
    "transformed_full = pd.concat([transformed_full,feature_full['booking_min_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_max_nights\n",
    "# no NAs\n",
    "# Capped everything above 2 months \n",
    "feature_full.loc[feature_full[\"booking_max_nights\"] >60, \"booking_max_nights\"] = 60\n",
    "#feature_full['booking_max_nights'].hist()\n",
    "transformed_full = pd.concat([transformed_full,feature_full['booking_max_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_cancel_policy\n",
    "# Seems fine\n",
    "#feature_full['booking_cancel_policy'].hist()\n",
    "\n",
    "cancel_cat = pd.get_dummies(feature_full['booking_cancel_policy'])\n",
    "transformed_full = pd.concat([transformed_full,cancel_cat],axis=1)\n",
    "transformed_full['super_strict_30']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_availability_30/60/90/365\n",
    "# No NAs in any of them\n",
    "# Transform availabilities into representing the availability of respective time periods (e.g. 365-90 day availability = availability for last 9 months)\n",
    "# Only execute once\n",
    "\n",
    "feature_full['booking_availability_365'] = feature_full['booking_availability_365']-feature_full['booking_availability_90']\n",
    "feature_full['booking_availability_90'] = feature_full['booking_availability_90'] - feature_full['booking_availability_60']\n",
    "feature_full['booking_availability_60'] = feature_full['booking_availability_60']- feature_full['booking_availability_30']\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['booking_availability_30']],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,feature_full['booking_availability_60']],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,feature_full['booking_availability_90']],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,feature_full['booking_availability_365']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_first, review_last\n",
    "# Could add difference between first and last, larger the better, could do some more sophisticated imputing. Chose simply the mode for now\n",
    "# Also skewed for now\n",
    "feature_full['reviews_first'] = pd.to_datetime(feature_full['reviews_first'])\n",
    "reviews_first_mode = feature_full['reviews_first'].mode()[0]\n",
    "feature_full['reviews_first'] = feature_full['reviews_first'].fillna(feature_full['reviews_first'].mode()[0])\n",
    "\n",
    "feature_full['reviews_last'] = pd.to_datetime(feature_full['reviews_last'])\n",
    "reviews_last_mode = feature_full['reviews_last'].mode()[0]\n",
    "feature_full['reviews_last'] = feature_full['reviews_last'].fillna(feature_full['reviews_last'].mode()[0])\n",
    "\n",
    "feature_full['reviews_first_since_scraped'] = (feature_full['property_scraped_at'] -feature_full['reviews_first']).dt.days.astype('int16')\n",
    "feature_full['reviews_last_since_scraped'] =  (feature_full['property_scraped_at'] -feature_full['reviews_last']).dt.days.astype('int16')\n",
    "\n",
    "feature_full = feature_full.drop(['reviews_first'], axis = 1) # Dropping old columns\n",
    "feature_full = feature_full.drop(['reviews_last'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['reviews_first_since_scraped']],axis=1)\n",
    "transformed_full = pd.concat([transformed_full,feature_full['reviews_last_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_rating\n",
    "reviews_rating_mode = feature_full['reviews_rating'].mode()[0]\n",
    "feature_full['reviews_rating']=feature_full['reviews_rating'].fillna(feature_full['reviews_rating'].mode()[0])\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['reviews_rating']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_acc\n",
    "reviews_acc_mode = feature_full['reviews_acc'].mode()[0]\n",
    "feature_full['reviews_acc']=feature_full['reviews_acc'].fillna(feature_full['reviews_acc'].mode()[0])\n",
    "\n",
    "transformed_full = pd.concat([transformed_full,feature_full['reviews_acc']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix column names\n",
    "transformed_full.columns = transformed_full.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = transformed_full.isna().any()\n",
    "print(nans.loc[nans==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fit best models on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(transformed_full)\n",
    "\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca4 = PCA(n_components=4)\n",
    "pca4.fit(transformed_full)\n",
    "\n",
    "full_PCA = pca4.transform(transformed_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create models\n",
    "lr = LinearRegression()\n",
    "\n",
    "#fit the models\n",
    "lr.fit(full_PCA,target_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lasso_large = Lasso(alpha=100)\n",
    "lasso_large.fit(transformed_full,target_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Create models\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(75,50),random_state=2023,max_iter=10000)\n",
    "\n",
    "#fit the models\n",
    "mlp.fit(transformed_full,target_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Prediction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Final Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train and test data sets (from GitHub, no need to add it manually)\n",
    "filepath = \"https://raw.githubusercontent.com/AnastasiaDv491/AA-datasets/main/test.csv\"\n",
    "\n",
    "feature_val = pd.read_csv(filepath)\n",
    "feature_val.head()\n",
    "\n",
    "ids = feature_val.property_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transform Final Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping this would be the same as the word doc table\n",
    "feature_val = feature_val.drop(['property_id','property_space','property_desc','property_neighborhood','property_notes','property_access',\n",
    "                                    \"property_interaction\",\"property_rules\",\"host_location\",\"host_about\",\"host_id\",\"property_sqfeet\"], axis = 1)\n",
    "feature_val = feature_val.drop([\"property_transit\"],axis=1)\n",
    "transformed_val = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All df to lower case -works\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def toLower(df):\n",
    "    df = df.apply(lambda x: x.str.lower() if x.dtype=='object' else x)\n",
    "    return df\n",
    "\n",
    "toLower = FunctionTransformer(toLower)\n",
    "toLower.fit_transform(feature_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property name: replaced by the word count & new column:missing? - works\n",
    "\n",
    "feature_val['property_name_miss'] = np.where(feature_val['property_name'].isna(), 1, 0)\n",
    "# Should refactor this because if run twice it throws an error\n",
    "feature_val.property_name = feature_val['property_name'].str.split().str.len()\n",
    "feature_val.property_name = feature_val.property_name.fillna(0)\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val.property_name],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,feature_val.property_name_miss],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property summary: new columns - summary_missing & property_summary_count\n",
    "## Missing = 'nan' or one word in summary\n",
    "str_df  = pd.DataFrame()\n",
    "str_df['condition'] = feature_val['property_summary'].str.match(r'\\A[\\w-]+\\Z')\n",
    "\n",
    "feature_val = pd.merge(str_df, feature_val, left_index=True, right_index=True)\n",
    "feature_val['property_summary_miss'] = np.where(feature_val['condition'].isna(), 1, 0)\n",
    "feature_val = feature_val.drop(['condition'], axis = 1)\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val.property_summary_miss],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Property_summary_count: remove numbers, punctuation \n",
    "feature_val['property_summary'] = feature_val['property_summary'].str.replace('\\d+', '')\n",
    "feature_val['property_summary'] = feature_val['property_summary'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "feature_val['property_summary_count'] = feature_val['property_summary'].str.split().str.len()\n",
    "feature_val = feature_val.drop(['property_summary'], axis = 1)\n",
    "feature_val.property_summary_count = feature_val.property_summary_count.fillna(0)\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val.property_summary_count],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_zipcode: replaced with mode\n",
    "feature_val['property_zipcode'] = feature_val['property_zipcode'] .fillna(zipcode_mode)\n",
    "\n",
    "feature_val.loc[feature_val['property_zipcode']=='11 20','property_zipcode']=1120\n",
    "\n",
    "freq_table=feature_val['property_zipcode'].value_counts()\n",
    "\n",
    "#encoding: preserve 11 most frequent categories\n",
    "feature_val.property_zipcode=feature_val.property_zipcode.astype(int)\n",
    "zipcode_cats = pd.get_dummies(feature_val.property_zipcode)\n",
    "\n",
    "zipcode_cats.columns\n",
    "sorted_zipcode_cats = zipcode_cats.loc[:,[1000, 1050, 1060, 2000, 1030,1040, 1190, 2018, 1180, 1070]]\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,sorted_zipcode_cats.iloc[:,:10]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_type: regrouped into - apartment, house, other\n",
    "threshold_percent = 3\n",
    "\n",
    "series = pd.value_counts(feature_val['property_type'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)\n",
    "\n",
    "feature_val = feature_val.assign(property_type = np.where(feature_val['property_type'].isin(series[mask].index),'Other', feature_val['property_type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode property_type\n",
    "type_cats = pd.get_dummies(feature_val['property_type'])\n",
    "transformed_val = pd.concat([transformed_val,type_cats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bathrooms: regroup into 1. None 2. One 3. More than one\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_val['property_bathrooms'] =feature_val['property_bathrooms'].fillna(bathrooms_mode)\n",
    "\n",
    "np.unique(feature_val['property_bathrooms'],return_counts=True)\n",
    "\n",
    "feature_val['property_bathrooms_cat']= 'One'\n",
    "feature_val.loc[(feature_val['property_bathrooms'] < 1) , 'property_bathrooms_cat'] = 'None'\n",
    "feature_val.loc[(feature_val['property_bathrooms'] > 1), 'property_bathrooms_cat'] = 'More than one'\n",
    "\n",
    "feature_val = feature_val.drop(['property_bathrooms'], axis = 1)\n",
    "feature_val['property_bathrooms_cat']\n",
    "\n",
    "bathroom_cats = pd.get_dummies(feature_val['property_bathrooms_cat'])\n",
    "transformed_val = pd.concat([transformed_val,bathroom_cats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bedrooms\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def NumToCategory(df, column,new_column, missing_value = 0):\n",
    "  df[column] =df[column].fillna(missing_value)\n",
    "\n",
    "  df.loc[(df[column] == 1), new_column] = 'One'\n",
    "  df.loc[(df[column] == 2), new_column] = 'Two'\n",
    "  df.loc[(df[column] == 3), new_column] = 'Three'\n",
    "  df.loc[(df[column] > 3), new_column] = 'More than three'\n",
    "\n",
    "  df = df.drop([column], axis = 1)\n",
    "  return df\n",
    "\n",
    "feature_val = NumToCategory(feature_val, 'property_beds', 'property_beds_cat')\n",
    "feature_val = NumToCategory(feature_val, 'property_bedrooms', 'property_bedrooms_cat')\n",
    "\n",
    "beds_dummies = pd.get_dummies(feature_val['property_beds_cat'])\n",
    "bedrooms_dummies = pd.get_dummies(feature_val['property_bedrooms_cat'])\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,beds_dummies,bedrooms_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_bed_type: ok\n",
    "feature_val['property_bed_type'].unique()\n",
    "# room type: ok\n",
    "pd.value_counts(feature_val['property_room_type'])\n",
    "\n",
    "\n",
    "roomtype_cat = pd.get_dummies(feature_val['property_room_type'])\n",
    "transformed_val = pd.concat([transformed_val,roomtype_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_amenities: count the number of amenities provided\n",
    "# remove nans by the mode \n",
    "\n",
    "# reviews_num: Power transform for skeweness \n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import skewtest\n",
    "\n",
    "\n",
    "feature_val['property_amenities'] =  feature_val['property_amenities'].str.split(', ').str.len()\n",
    "feature_val['property_amenities'] = feature_val['property_amenities'].fillna(amenities_mode)\n",
    "\n",
    "def powerTransform(df, column):\n",
    "  col  = np.array( df[column]).reshape(-1, 1)\n",
    "  pt = PowerTransformer(method='yeo-johnson', standardize=True,) \n",
    "  fit = pt.fit(col)\n",
    "  fit = pt.transform(col)\n",
    "  df[column] = fit\n",
    "  return df\n",
    "\n",
    "feature_val = powerTransform(feature_val,'property_amenities')\n",
    "feature_val = powerTransform(feature_val,'reviews_num')\n",
    "\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val.property_amenities],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,feature_val.reviews_num],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_cleanliness\n",
    "# Good: above 5, Bad: below 5, None: average (could be set to \"Missing\", but people who find a property okay usually dont leave reviews, hence, average. Missing might be misleading)\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def reviewRecoding(df, column, new_column):\n",
    "  df[new_column] = df[column].fillna('Average')\n",
    "  df.loc[(df[column] <= 5) , new_column] = 'Bad'\n",
    "  df.loc[(df[column] > 5), new_column] = 'Good'\n",
    "  df = df.drop([column], axis = 1)\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_val = reviewRecoding(feature_val, 'reviews_cleanliness', 'reviews_cleanliness_n')\n",
    "feature_val = reviewRecoding(feature_val, 'reviews_checkin', 'reviews_checkin_n')\n",
    "feature_val = reviewRecoding(feature_val, 'reviews_location', 'reviews_communication_n')\n",
    "feature_val = reviewRecoding(feature_val, 'reviews_communication', 'reviews_communication_n')\n",
    "feature_val = reviewRecoding(feature_val, 'reviews_value', 'reviews_value_n')\n",
    "\n",
    "cleanliness_cat = pd.get_dummies(feature_val['reviews_cleanliness_n'])\n",
    "checkin_cat = pd.get_dummies(feature_val['reviews_checkin_n'])\n",
    "location_cat = pd.get_dummies(feature_val['reviews_communication_n'])\n",
    "communication_cat = pd.get_dummies(feature_val['reviews_communication_n'])\n",
    "value_cat = pd.get_dummies(feature_val['reviews_value_n'])\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,cleanliness_cat],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,checkin_cat],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,location_cat],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,communication_cat],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,value_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_per_month (avg): right skewed - log transform\n",
    "#add 1 to all to avoid log problems\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "feature_val['reviews_per_month'] =feature_val['reviews_per_month'].fillna(reviews_per_month_mode)\n",
    "feature_val['reviews_per_month'] =feature_val['reviews_per_month']+1\n",
    "def reviewsLog(df, feature):\n",
    "  logTr = ColumnTransformer(transformers=[('lg', FunctionTransformer(np.log),[feature])])\n",
    "  log = logTr.fit_transform(df)\n",
    "  df[feature] = log\n",
    "\n",
    "  return df\n",
    "\n",
    "feature_val = reviewsLog(feature_val, 'reviews_per_month')\n",
    "feature_val.reviews_per_month = feature_val.reviews_per_month.fillna(0)\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val.reviews_per_month],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_scraped_at\n",
    "feature_val['property_scraped_at']= pd.to_datetime(feature_val['property_scraped_at'])\n",
    "feature_val['scraped_weekday']=feature_val['property_scraped_at'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode property_scraped_at\n",
    "start_year = 2017\n",
    "days_since_2017 = (feature_val['property_scraped_at'] - pd.Timestamp(str(start_year))).dt.days\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,days_since_2017],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_last_updated\n",
    "# Naturally last_updated is a string :))))))\n",
    "\n",
    "def last_updated_conversion(update_var:str):\n",
    "  # 3 Buckets evenly distributed based on val, 2 for > 3 months, 0 for updated yesterday/today\n",
    "  if \"never\" in update_var:\n",
    "    return \"3 months or more\"\n",
    "  elif \"month\" in update_var:\n",
    "    if int(update_var.split(\" \")[0])>=3:\n",
    "      return \"3 months or more\"\n",
    "    else:\n",
    "      return \"1 week to 3 months\"\n",
    "  elif \"week\" in update_var:\n",
    "    return \"1 week to 3 months\"\n",
    "  else: # This catches yesterday etc. is actually equivalent to \"day\" in update_var\n",
    "    return \"Within days\"\n",
    "\n",
    "feature_val[\"property_last_updated_bucket\"] = [last_updated_conversion(x) for x in feature_val[\"property_last_updated\"]] \n",
    "feature_val = feature_val.drop([\"property_last_updated\"], axis = 1) # Dropping old columns\n",
    "\n",
    "last_updated_cat = pd.get_dummies(feature_val['property_last_updated_bucket'])\n",
    "transformed_val = pd.concat([transformed_val,last_updated_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_since\n",
    "feature_val['host_since']=pd.to_datetime(feature_val['host_since'])\n",
    "feature_val['host_since'] = feature_val['host_since'].fillna(host_since_mean) # Simply imputing with mean, only had 1 NA anyways\n",
    "feature_val['host_since_scraped'] = feature_val['property_scraped_at']-feature_val['host_since']\n",
    "feature_val['host_since_scraped'] = feature_val['host_since_scraped'].dt.days.astype('int16')\n",
    "#feature_val['host_since_scraped'].hist()\n",
    "feature_val = feature_val.drop(['host_since'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['host_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_response_time\n",
    "# Given 1k NAN decided to add extra missing category as never having received messages might be a signal, otherwise seems fine\n",
    "feature_val['host_response_time']=feature_val['host_response_time'].fillna(value=\"Missing\",inplace=True)\n",
    "#feature_val['host_response_time'].hist()\n",
    "\n",
    "host_response_cat = pd.get_dummies(feature_val['host_response_time'])\n",
    "transformed_val = pd.concat([transformed_val,host_response_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_response_rate\n",
    "# Impute 100% response rate for never having received a request which is \"fair\"\n",
    "feature_val['host_response_rate']=feature_val['host_response_rate'].fillna(value=100)\n",
    "# Similar to Rating skewness\n",
    "#feature_val['host_response_rate'].hist()\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['host_response_rate']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_nr_listings, host_nr_listings_total\n",
    "# Decided to drop host_nr_listings_total for now as it is basically the same information as host_nr_listings in the valing set,\n",
    "# if one wants to squeeze more information out one could take the difference between the two as an extra feature\n",
    "# Basically everyone only has 1 property\n",
    "if \"host_nr_listings_total\" in feature_val.columns:\n",
    "  feature_val = feature_val.drop(['host_nr_listings_total'], axis = 1)\n",
    "\n",
    "feature_val.loc[(feature_val['host_nr_listings'] <= 1) , 'host_nr_listings_cat'] = 'One or less'\n",
    "feature_val.loc[(feature_val['host_nr_listings'] > 1) & (feature_val['host_nr_listings'] <=3), 'host_nr_listings_cat'] = 'Two to Three'\n",
    "feature_val.loc[(feature_val['host_nr_listings'] > 3), 'host_nr_listings_cat'] = 'More than 3'\n",
    "\n",
    "#feature_val['host_nr_listings_cat'].hist()\n",
    "feature_val = feature_val.drop(['host_nr_listings'], axis = 1) # Dropping old columns\n",
    "\n",
    "nr_listings_cat = pd.get_dummies(feature_val['host_nr_listings_cat'])\n",
    "transformed_val = pd.concat([transformed_val,nr_listings_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_verified\n",
    "# csv sheet including everything that is \"verified\" e.g. email,phone reviews etc.\n",
    "# Same procedure as with property_summary\n",
    "feature_val['host_verified_count'] = feature_val['host_verified'].str.split(\",\").str.len()\n",
    "#feature_val['host_verified_count'].hist()\n",
    "feature_val = feature_val.drop(['host_verified'], axis = 1) # Dropping old columns\n",
    "\n",
    "feature_val['host_verified_count'] = feature_val['host_verified_count'].fillna(0)\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['host_verified_count']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_price_covers \n",
    "# Number of people that can live in the property for the price\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#feature_val['booking_price_covers'].hist()\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['booking_price_covers']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_min_nights\n",
    "# No NAs\n",
    "# Choosing to drop anything that requires booking more than 1 month which is only 37 observations. It should be noted that this will lead to us losing\n",
    "# some information on longterm rentals but I think this is preferable, in particular as these might be faulty observations anyways\n",
    "# Again similar to raitings etc. 1 very large category and then everything else. Tempted to code as 0/1\n",
    "\n",
    "#for now: cap at 31\n",
    "feature_val.loc[feature_val[\"booking_max_nights\"] >31, \"booking_max_nights\"] = 31\n",
    "feature_val['booking_min_nights'].hist()\n",
    "transformed_val = pd.concat([transformed_val,feature_val['booking_min_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_max_nights\n",
    "# no NAs\n",
    "# Capped everything above 2 months \n",
    "feature_val.loc[feature_val[\"booking_max_nights\"] >60, \"booking_max_nights\"] = 60\n",
    "#feature_val['booking_max_nights'].hist()\n",
    "transformed_val = pd.concat([transformed_val,feature_val['booking_max_nights']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_cancel_policy\n",
    "# Seems fine\n",
    "#feature_val['booking_cancel_policy'].hist()\n",
    "\n",
    "cancel_cat = pd.get_dummies(feature_val['booking_cancel_policy'])\n",
    "transformed_val = pd.concat([transformed_val,cancel_cat],axis=1)\n",
    "transformed_val['super_strict_30']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_availability_30/60/90/365\n",
    "# No NAs in any of them\n",
    "# Transform availabilities into representing the availability of respective time periods (e.g. 365-90 day availability = availability for last 9 months)\n",
    "# Only execute once\n",
    "\n",
    "feature_val['booking_availability_365'] = feature_val['booking_availability_365']-feature_val['booking_availability_90']\n",
    "feature_val['booking_availability_90'] = feature_val['booking_availability_90'] - feature_val['booking_availability_60']\n",
    "feature_val['booking_availability_60'] = feature_val['booking_availability_60']- feature_val['booking_availability_30']\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['booking_availability_30']],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,feature_val['booking_availability_60']],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,feature_val['booking_availability_90']],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,feature_val['booking_availability_365']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_first, review_last\n",
    "# Could add difference between first and last, larger the better, could do some more sophisticated imputing. Chose simply the mode for now\n",
    "# Also skewed for now\n",
    "feature_val['reviews_first'] = pd.to_datetime(feature_val['reviews_first'])\n",
    "feature_val['reviews_first'] = feature_val['reviews_first'].fillna(reviews_first_mode)\n",
    "\n",
    "feature_val['reviews_last'] = pd.to_datetime(feature_val['reviews_last'])\n",
    "feature_val['reviews_last'] = feature_val['reviews_last'].fillna(reviews_last_mode)\n",
    "\n",
    "feature_val['reviews_first_since_scraped'] = (feature_val['property_scraped_at'] -feature_val['reviews_first']).dt.days.astype('int16')\n",
    "feature_val['reviews_last_since_scraped'] =  (feature_val['property_scraped_at'] -feature_val['reviews_last']).dt.days.astype('int16')\n",
    "\n",
    "feature_val = feature_val.drop(['reviews_first'], axis = 1) # Dropping old columns\n",
    "feature_val = feature_val.drop(['reviews_last'], axis = 1) # Dropping old columns\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['reviews_first_since_scraped']],axis=1)\n",
    "transformed_val = pd.concat([transformed_val,feature_val['reviews_last_since_scraped']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_rating\n",
    "feature_val['reviews_rating']=feature_val['reviews_rating'].fillna(reviews_rating_mode)\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['reviews_rating']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_acc\n",
    "feature_val['reviews_acc']=feature_val['reviews_acc'].fillna(reviews_acc_mode)\n",
    "\n",
    "transformed_val = pd.concat([transformed_val,feature_val['reviews_acc']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix column names\n",
    "transformed_val.columns = transformed_val.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = transformed_val.isna().any()\n",
    "print(nans.loc[nans==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_full.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_PCA = pca4.transform(transformed_val)\n",
    "\n",
    "price_pred_lr = lr.predict(val_PCA)\n",
    "\n",
    "#print(price_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to csv\n",
    "\n",
    "price_pred_lr=price_pred_lr.flatten('C')\n",
    "price_pred_lr=price_pred_lr.round(decimals=4)\n",
    "df=pd.DataFrame({'ID':ids,'PRED':price_pred_lr})\n",
    "df\n",
    "df.to_csv('price_pred_lr.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pred_lasso = lasso_large.predict(transformed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pred_lasso=price_pred_lasso.flatten('C')\n",
    "price_pred_lasso=price_pred_lasso.round(decimals=4)\n",
    "df=pd.DataFrame({'ID':ids,'PRED':price_pred_lasso})\n",
    "df\n",
    "df.to_csv('price_pred_lasso.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pred_mlp = mlp.predict(transformed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pred_mlp=price_pred_mlp.flatten('C')\n",
    "price_pred_mlp=price_pred_mlp.round(decimals=4)\n",
    "df=pd.DataFrame({'ID':ids,'PRED':price_pred_mlp})\n",
    "df\n",
    "df.to_csv('price_pred_mlp.csv',index=False,sep=',')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0SLjt5KOjT0n",
    "Puz6mw2NRBz7",
    "qszpOlYr28v-",
    "liCQnLyeJBHV"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
